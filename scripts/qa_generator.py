# ğŸ“ scripts/qa_generator.py

import os
import json
import openai
from dotenv import load_dotenv

# .env ã‹ã‚‰ OPENAI_API_KEY ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# å…¥å‡ºåŠ›ãƒ‘ã‚¹
INPUT_PATH = "./data/processed/chunks.json"
OUTPUT_PATH = "./data/processed/qa_data.json"
os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)

# ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿
with open(INPUT_PATH, "r", encoding="utf-8") as f:
    chunks = json.load(f)

# GPTã«å•ã„åˆã‚ã›ã‚‹é–¢æ•°
def generate_qa(chunk_text):
    prompt = f"""
ä»¥ä¸‹ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€1ã¤ã®è³ªå•ã¨ãã®å›ç­”ã‚’æ—¥æœ¬èªã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

--- å†…å®¹ ---
{chunk_text}
--- å‡ºåŠ›å½¢å¼ ---
{{"question": "...", "answer": "..."}}
"""
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯é‰„é“ä¿å®ˆã®å°‚é–€å®¶ã§ã™ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    # å¿œç­”ã‹ã‚‰JSONéƒ¨åˆ†ã ã‘æŠ½å‡º
    content = response.choices[0].message.content.strip()
    return json.loads(content)

# å…¨ãƒãƒ£ãƒ³ã‚¯ã«Q&Aã‚’ç”Ÿæˆ
qa_data = []
for chunk in chunks:
    try:
        qa = generate_qa(chunk["content"])
        qa["id"] = chunk["id"]
        qa_data.append(qa)
    except Exception as e:
        print(f"âŒ chunk {chunk['id']} failed: {e}")

# ä¿å­˜
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(qa_data, f, ensure_ascii=False, indent=2)

print(f"âœ… Q&Aç”Ÿæˆå®Œäº†: {len(qa_data)} ä»¶ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {OUTPUT_PATH}")
